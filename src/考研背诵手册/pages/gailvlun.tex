\chapter{概率论}

\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbD}{\mathbb{D}}

\section{随机事件和概率}

\subsection{大纲要求}

1. 了解样本空间（基本事件空间）的概念，理解随机事件的概念，掌握事件的关系及运算。

2. 理解概率、条件概率的概念，掌握概率的基本性质，会计算古典型概率和几何型概率，掌握概率的加法公式、减法公式、乘法公式、全概率公式以及贝叶斯（Bayes）公式。

3. 理解事件独立性的概念，掌握用事件独立性进行概率计算；理解独立重复试验的概念，掌握计算有关事件概率的方法。

\subsection{概率}

\begin{definition}
	设 $A,B$ 是两个事件且 $\bbP(A) > 0$，我们称在已知 $A$ 发生的条件下事件 $B$ 发生的概率为条件概率，记作
	\[ \bbP(B | A) = \frac{\bbP(AB)}{\bbP(A)} \]
\end{definition}

假如 $\bbP(B \mid A) > \bbP(B)$，我们可以说事件 $A$ 促进了事件 $B$ 的发生。反之 $\bbP(B \mid A) = \bbP(A)$，则 $B$ 的发生对 $A$ 无影响。

若两事件 $A, B$ 满足
\[ \bbP(A B) = \bbP(A) \bbP(B) \]
则称 $A, B$ 独立。

设一列事件 $A_1, A_2, \cdots$，假如从中取出任意有限个都成立
\[ \bbP(A_{i1} \cdots A_{im}) = \bbP(A_{i1}) \cdots \bbP(A_{im}) \]
那么称事件 $A_1, A_2, \cdots$ 相互独立。注意与两两独立的区别。


\begin{theorem}[全概率公式]
	设 $B_1, \cdots$ 为一列事件，他们两两互斥且每次试验至少发生一个。有时称这种性质为“完备事件群”。那么对任意事件 $A$ 有
	\[ \bbP(A) = \sum_{i=1}^n \bbP(B_i)\bbP(A \mid B_i) \]
\end{theorem}

\begin{proof}
	显然
	\[ \bbP(A) = \sum_{i=1}^n \bbP(B_i A) = \sum_{i=1}^n \bbP(B_i)\bbP(A \mid B_i) \]
\end{proof}


\begin{theorem}[贝叶斯公式]
	对 $n$ 个两两不相容事件 $\seq{A}{N}$，则对事件 $B$ 有
	\[ \bbP\left(A_j | B\right) = \frac{\bbP(A_j)\bbP(B | A_j)}{\sum\limits_{i=1}^n \bbP(A_i)\bbP(B | A_i) } \]
\end{theorem}

\begin{proof}
	显然
	\[ \bbP\left(A_j | B\right) = \frac{\bbP(A_j B)}{\bbP(B)} = \frac{\bbP(A_j)\bbP(B | A_j)}{\sum\limits_{i=1}^n \bbP(A_i)\bbP(B | A_i) } \]
\end{proof}

\begin{note}
	若 $\bbP(AB) = 0$，不意味着 $AB = \varnothing$。比如 $[0,1]\cap [1,2] = \{1\}$，但概率是 $0$。
\end{note}


\begin{theorem}[伯努利定理]
	假如一次试验中，事件 $A$ 发生的概率为 $p(0 < p < 1)$，则 $n$ 重伯努利试验中，事件 $A$ 恰好发生 $k$ 次的概率为
	\[ \binom{n}{k} p^k (1 - p)^{n-k} \]
\end{theorem}

\section{随机变量及其分布}

\subsection{大纲要求}

1. 理解随机变量的概念，理解分布函数
\[ F(x) = P\{X \leqslant x\}, \quad -\infty < x < +\infty \]
的概念及性质，会计算与随机变量相联系的事件的概率。

2. 理解离散型随机变量及其概率分布的概念，掌握 $0-1$ 分布、二项分布 $B(n, p)$、几何分布、超几何分布、泊松（Poisson）分布 $P(\lambda)$ 及其应用。

3. 了解泊松定理的结论和应用条件，会用泊松分布近似表示二项分布。

4. 理解连续型随机变量及其概率密度的概念，掌握均匀分布 $U(a, b)$、 正态分布 $N(\mu, \sigma^2)$、指数分布及其应用，其中参数为 $\lambda(\lambda > 0)$ 的指数分布 $E(\lambda)$ 的概率密度为
\[ f(x) = \begin{cases} \lambda e^{-\lambda x}, & x > 0 \\ 0, &x \leqslant 0 \end{cases} \]
5. 会求随机变量函数的分布。

\subsection{常见分布}

\paragraph{二项分布 $X \sim B(n, p)$}
如果 $X$ 的概率分布为
\[ \bbP\{X = k\} = \binom{n}{k} p^k(1 - p)^{n-k}. \quad k = 0, 1, \cdots, n \]
组合意义是 $n$ 重伯努利实验中事件 $A$ 发生的次数，其中 $\bbP(A) = p$。其 $\bbE [X] = np, \bbD [X] = np(1-p)$。

\paragraph{泊松分布 $X \sim P(\lambda)$}
如果 $X$ 的概率分布为
\[ \bbP\{X = k\} = \frac{\lambda^k}{k!} \ee^{-\lambda} , \quad k = 0, 1, \cdots \]
其 $\bbE [X] = \lambda, \bbD [X] = \lambda$。


\paragraph{几何分布 $X \sim G(p)$}
如果 $X$ 的概率分布为
\[ \bbP\{X=k\} = (1 - p)^{k-1}p, \quad k = 1, 2, \cdots \]
其 $\bbE [X] = \frac{1}{p}, \bbD [X] = \frac{1-p}{p^2}$。

\paragraph{超几何分布 $X \sim H(n, N, M)$}
如果 $X$ 的概率分布为
\[ \bbP\{X = k\} = \frac{\binom{M}{k} \binom{N - M}{n - k}}{\binom{N}{n}}, \quad \max(0, n - N + M) \leqslant k \leqslant \min(M, n) \]
其组合意义是在 $N$ 个产品 $M$ 个不合格产品，从中取出 $n$ 个次品数为 $k$。

\paragraph{均匀分布 $X \sim U(a, b)$}
如果 $X$ 的概率密度函数为
\[ f(x) = \frac{1}{b - a}, \quad a < x < b \]
其 $\bbE [X] = \frac{a+b}{2}, \bbD [X] = \frac{(b-a)^2}{12}$。

\paragraph{指数分布 $X \sim E(\lambda)$}
如果 $X$ 的概率密度函数为
\[ f(x) = \lambda \ee^{-\lambda x}, \quad x > 0 \]
其 $\bbE [X] = \frac{1}{\lambda}, \bbD [X] = \frac{1}{\lambda^2}$。

\paragraph{正态分布 $X \sim N(\mu, \sigma^2)$}
如果 $X$ 的概率密度为
\[ f(x) = \frac{1}{\sigma} \varphi\left(\frac{x - \mu}{\sigma}\right) = \frac{1}{\sqrt{2 \pi}  \sigma} \exp \left(- \frac{(x - \mu)^2}{2\sigma^2}  \right), \quad \varphi(x) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x^2}{2}\right) \]
如果 $X \sim N(\mu, \sigma^2)$，则 $\frac{X - \mu}{\sigma} \sim N(0, 1)$。
其 $\bbE [X] = \mu, \bbD [X] = \sigma^2$。


\section{多维随机变量及其分布}

\subsection{大纲要求}

1. 理解多维随机变量的概念，理解多维随机变量的分布的概念和性质，理解二维离散型随机变量的概率分布、边缘分布和条件分布，理解二维连续型随机变量的概率密度、边缘密度和条件密度，会求与二维随机变量相关事件的概率。

2. 理解随机变量的独立性及不相关性的概念，掌握随机变量相互独立的条件。

3. 掌握二维均匀分布，了解二维正态分布 $N(\mu_1, \mu_2; \sigma_1^2, \sigma_2^2; \rho)$ 的概率密度，理解其中参数的概率意义。

4. 会求两个随机变量简单函数的分布，会求多个相互独立随机变量简单函数的分布。

\subsection{多维随机变量及其分布}

\paragraph{离散型}

假如每个分量是离散型的，那么称 $X$ 是 $n$ 维离散型随机变量。

\begin{definition}
	以 $\{a_{i1}, \cdots\}$ 记 $X_i$ 的全部可能值，则事件的概率
	\[ p(\seq{j}{n}) = \bbP\{X_1 = a_{1j_1}, \cdots, X_n = a_{n j_n}\} \]
	为随机变量的概率函数。显然应该满足条件
	\[ p(j_1, \cdots) \geqslant 0, \quad \sum_{j_n} \cdots \sum_{j_1} p(j_1, \cdots)  =1 \]
\end{definition}

\paragraph{连续型}
连续型随机变量不能简单的定义为各分量都是“一维连续型随机变量的那种”。考虑 $X_1 = X_2$，则 $(X_1, X_2)$ 仅在对角线处有值，故不可能存在概率密度函数。

\begin{definition}
	设 $f(\seq{x}{n})$ 是定义在 $\mathbb{R}^n$ 上的非负函数，使得对 $\mathbb{R}^n$ 中的任何集合 $A$，有
	\[ \bbP\{X \in A\} = \int_A \cdots \int f(\seq{x}{n}) \d x_1 \cdots \d x_n \]
	则称 $f$ 是 $X$ 的概率密度函数。显然当 $\bbP\{ X \in \mathbb{R}^n\} = 1$。
\end{definition}

也可以定义分布函数
\[ F(\seq{x}{n}) = \bbP\{X_1 \leqslant x_1, \cdots, X_n \leqslant x_n \} \]

对于二维 $(X, Y)$，若 $f$ 在点 $(x, y)$ 处连续，则
\[ \frac{\partial^2 F(x, y)}{\partial x \partial y} = f(x, y) \]
反之若 $F(x, y)$ 连续可导，则此式是其概率密度。

\paragraph{边缘分布}

对任意的 $n$ 个实数，$x_1, x_2, \cdots, x_n$，称 $n$ 元函数
\[ F(\seq{x}{n}) = \bbP\{X_1 \leqslant x_1, X_2 \leqslant x_2, \cdots. X_n \leqslant x_n\} \]
为多维随机变量 $(\seq{x}{n})$ 的联合分布函数，记为 $(\seq{x}{n}) \sim F(\seq{x}{n})$。

\begin{itemize}
	\item $F(x, y)$ 是 $x, y$ 的单调不减函数。
	\item $F(x, y)$ 是 $x, y$ 的右连续函数。
	\item $F(-\infty, y) = F(x, -\infty) = F(-\infty, -\infty) = 0$，$F(+\infty, +\infty) = 1$。
	\item 对任意 $x_1 < x_2, y_1 < y_2$，有
	      \[ \bbP\{x_1 < X \leqslant x_2, y_1 Y \leqslant y_2\} = F(x_2, y_2) - F(x_2, y_1) - F(x_1, y_2) + F(x_1, y_1) \geqslant 0 \]
\end{itemize}

设其联合分布函数为 $F(x,y)$，定义边缘分布函数
\[ F_X(x) = \bbP\{X \leqslant x\} = F(x, +\infty) \]
同理，有 $F_Y(y) = F(+\infty, y)$。



\subsection{二维正态分布}

设二维随机变量的概率密度为
\[ f(x, y) = \frac{1}{2 \pi \sigma_1 \sigma_2 \sqrt{1 - \rho^2}} \exp \left( -\frac{1}{2(1-p^2)} \left(\frac{(x-\mu_1)^2}{\sigma_1^2} + \frac{(y-\mu_2)^2}{\sigma_2^2} - 2 \rho\frac{(x-\mu_1)(y-\mu_2)}{\sigma_1 \sigma_2}\right) \right) \]
记为 $N(\mu_1, \mu_2; \sigma_1, \sigma_2; \rho)$。

当 $\rho = 0$，可以看作 $X \sim N(\mu_1, \sigma_1^2), Y \sim N(\mu_2, \sigma_2^2)$。

\subsection{两个随机变量简单函数}

对于随机变量 $(X, Y)$，求 $g(X, Y)$ 的概率密度和分布函数有两种方法：

\subsubsection*{定义法}

设其概率密度函数为 $f(x, y)$，先求 $Z = g(X, Y)$ 的概率密度
\[ F_Z(z) = \bbP\{Z \leqslant z\} = \bbP\{g(X,Y) \leqslant z\} = \iint_{g(X,Y) \leqslant z} f(x, y) \d x \d y \]
其概率密度则为 $f_Z(z) = \frac{\d F_z(z)}{\d z}$。

\subsubsection*{公式法}

对于 $Z = X + Y$，则
\[ f_Z(z) = \int_{-\infty}^{+\infty} f(z - y, y) \d y = \int_{-\infty}^{+\infty} f(x, z - x) \d x \]
对于 $Z = XY$，则
\[ f_Z(z) = \int_{-\infty}^{+\infty} \frac{1}{|x|}f\left(x, \frac{z}{x}\right) \d y = \int_{-\infty}^{+\infty} \frac{1}{|y|}f\left(\frac{z}{y}, y\right) \d x \]
对于 $Z = \frac{X}{Y}$，则
\[ f_Z(z) = \int_{-\infty}^{+\infty} |y|f(yz, y) \d y \]
对于 $Z = \max\{X, Y\}$，则
\[ F_Z(z) = F_X(z)F_Y(z) \]
对于 $Z = \min\{X, Y\}$，则
\[ F_Z(z) = 1 - (1 - F_X(z))(1 - F_Y(z)) \]

\section{随机变量的数字特征}

\subsection{大纲要求}

1. 理解随机变量数字特征（数学期望、方差、标准差、矩、协方差、相关系数）的概念，会运用数字特征的基本性质，并掌握常用分布的数字特征。

2. 会求随机变量函数的数学期望。

\subsection{期望和方差}

\subsection*{期望}

设 $X$ 是随机变量，其分布列为 $p_i = \bbP\{X = x_i\}$，记
\[ \bbE [X] = \sum_{i=1}^\infty x_i p_i, \quad \bbE[g(X)] = \sum_{i=1}^\infty g(x_i) p_i \]
为随机变量 $X$ 的数学期望。若 $X$ 是连续型随机变量，则记
\[ \bbE [X] = \int_{-\infty}^{+\infty} x f(x) \d x, \quad \bbE[g(X)] = \int_{-\infty}^{+\infty} g(x) f(x) \d x \]
为其期望

期望的性质
\begin{itemize}
	\item 对于常数 $C$，有 $\bbE[C] = C$；
	\item 线性性：$\bbE[\sum k_iX_i] = \sum k_i \bbE [X]$；
	\item 独立性：如果 $X_1, \cdots, X_n$ 相互独立，$\bbE[X_1X_2\cdots X_n] = \bbE[X_1] \cdots \bbE[X_n]$。
\end{itemize}

\subsection*{方差}

我们记 $\bbE[(X - \bbE X)^2]$ 为 $X$ 的方差，有
\[ \bbD [X] = \bbE[(X - \bbE X)^2] = \bbE X^2 - (\bbE X)^2 \]
称 $\sqrt{\bbD [X]}$ 为 $X$ 的标准差，或者均方差，记为 $\sigma(X)$。

若 $X$ 是离散型随机变量，则
\[ \bbD [X] = \sum_{i=1}^{\infty} (x_i - \bbE X)^2 p_i \]
连续性随机变量，则
\[ \bbD [X] = \int_{-\infty}^{\infty} (x-\bbE X)^2 f(x) \d x \]

方差的性质
\begin{itemize}
	\item 对于常数 $C$，有 $\bbD[C] = C^2$；
	\item 线性性：如果 $X_1, \cdots, X_n$ 相互独立，$\bbD[\sum k_iX_i] = \sum k_i^2 \bbE [X]$。
\end{itemize}

\subsection{协方差和相关系数}

\subsection*{矩}
\begin{itemize}
	\item $k$ 阶原点矩：$\bbE[X^k]$；
	\item $k$ 阶中心距：$\bbE[(X - \bbE X)^k]$；
	\item $k + l$ 阶混合矩：$\bbE[X^k Y^l]$；
	\item $k + l$ 阶中心距：$\bbE[(X - \bbE X)^k (Y - \bbE Y)^l]$；
\end{itemize}

\subsection*{协方差}

\newcommand{\opCov}{\operatorname{Cov}}

我们定义 $(X, Y)$ 的协方差为
\[ \opCov(X, Y) = \bbE[(X - \bbE X)(Y - \bbE Y))] = \bbE(XY) - \bbE X \cdot \bbE Y \]

称 $\rho_{XY} = \frac{\opCov(X, Y)}{\sqrt{\bbD [X] \bbD(Y)}}$ 为 $X, Y$ 的相关系数。

\begin{theorem}[切比雪夫不等式]
	如果随机变量 $X$ 的期望 $\bbE [X]$ 和方差 $\bbD [X]$ 存在，则对任意 $\eps > 0$ 有
	\[ \bbP\{|X - \bbE [X]| < \eps\} \geqslant 1 - \frac{\bbD [X]}{\eps^2} \]
\end{theorem}



\subsection*{协方差的性质}

\begin{itemize}
	\item $\opCov(X, Y) = \opCov(Y, X)$；
	\item $\opCov(X, X) = \bbD X$；
	\item $\opCov(aX, bY) = ab \opCov(X, Y)$；
	\item $\opCov(X_1 + X_2, Y) = \opCov(X_1, Y) + \opCov(X_2, Y)$；
	\item $\bbD(X \pm Y) = \bbD X + \bbD Y \pm 2Cov(X, Y)$；
\end{itemize}

$X, Y$ 相互独立可以推出不相关，但反之不行；

$X, Y$ 不相关与下列命题互推
\begin{itemize}
	\item $\rho_{XY} = 0$；
	\item $\opCov(X, Y) = 0$
	\item $\bbE[XY] = \bbE X \cdot \bbE Y$；
	\item $\bbE(X \pm Y) = \bbE X + \bbE Y$。
\end{itemize}

\subsection{\texorpdfstring{$\Gamma$ 函数}{Γ 函数}}

$\Gamma$ 函数是一有力的工具，其定义为
\[ \Gamma(z) = \int_{0}^{+\infty} t^{z-1} \ee^{-t} \d t \]
其具有递推式和余元公式
\[ \Gamma(z + 1) = z \Gamma(z), \quad \Gamma(z) \Gamma(1-z) = \frac{\pi}{\sin \pi z} \]
特殊的，$\Gamma(n + 1) = n!$，和 $\Gamma(\frac{1}{2}) = \sqrt{\pi}$。不难化简
\[ I_n = \int_{0}^{+\infty} x^n \ee^{-x^2} \d x = \frac{1}{2} \int_{0}^{+\infty} \left(x^2\right)^{\frac{n+1}{2}} \ee^{-x^2} \d \left(x^2\right) = \frac{1}{2} \Gamma\left(\frac{n+1}{2}\right)  \]

\section{大数定律和中心极限定理}

\subsection{大纲要求}

1. 了解切比雪夫不等式。

2. 了解切比雪夫大数定律、伯努利大数定律和辛钦大数定律（独立同分布随机变量序列的大数定律）。

3. 了解棣莫弗-拉普拉斯定理（二项分布以正态分布为极限分布）和列维-林德伯格定理（独立同分布随机变量序列的中心极限定理）。

\subsection{神秘公式}

设随机变量 $X$ 与随机变量序列 $\{X_n\}$，如果对任意的 $\eps > 0$ 有
\[ \lim_{n \to \infty} \bbP\{|X_n - X| < \eps \} =1 \]
则称随机变量序列 $\{X_n\}$ 依概率收敛于随机变量 $X$，记为
\[ \lim_{n \to \infty} X_n = X(P), \quad \text{或}\ X_n \stackrel{P}{\longrightarrow} X(n \to \infty) \]

\begin{theorem}[切比雪夫大数定律]
	设 $\{X_n\}$ 是相互独立的随机变量序列，如果方差 $\bbD [X]$ 存在且有一致有上界，则 $\{X_n\}$ 服从大数定律
	\[ \frac{1}{n} \sum_{i=1}^n X_i \stackrel{P}{\longrightarrow} \frac{1}{n} \sum_{i=1}^n \bbE(X_i) \]
\end{theorem}

\begin{theorem}[伯努利大数定律]
	假设 $\mu_n$ 是 $n$ 重伯努利实验中时间 $A$ 发生的次数，在每次实验中 $A$ 发生的概率为 $p(0 < p < 1)$，则
	\[ \frac{\mu_n}{n} \stackrel{P}{\longrightarrow} p \]
\end{theorem}

\begin{theorem}[辛钦大数定律]
	设 $\{X_n\}$ 是独立同分布的随机变量序列，如果 $\bbE(X_i) = \mu$ 存在，则
	\[ \frac{1}{n} \sum_{i=1}^n X_i \stackrel{P}{\longrightarrow} \mu \]
\end{theorem}

\begin{theorem}[列维 - 林德伯格定理]
	假设 $\{X_n\}$ 是独立同分布的随机变量序列，如果
	\[ \bbE(X_i) = \mu, \bbD(X_i) = \sigma^2 > 0 \]
	存在，则对任意的实数 $x$ 有
	\[ \lim_{n \to \infty} \bbP\left\{ \frac{\sum_{i=1}^n X_i - n \mu}{\sigma \sqrt{n}} \leqslant x \right\} = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^x \exp\left(-\frac{t^2}{2}\right) \d t = \Phi(x)  \]
\end{theorem}

\begin{theorem}[棣莫弗 - 拉普拉斯定理]
	设随机变量 $Y_n \sim B(n, p)$，其中 $0 < p < 1$ 且 $n > 1$，则对任意的实数 $x$，有
	\[ \lim_{n \to \infty} \bbP\left\{ \frac{Y_n - np}{\sqrt{np(1 - p)}} \leqslant x \right\} = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^x \exp\left(-\frac{t^2}{2}\right) \d t = \Phi(x)  \]
\end{theorem}


\section{数理统计的基本概念}

\subsection{大纲要求}

1. 理解总体、简单随机样本、统计量、样本均值、样本方差及样本矩的概念，其中样本
方差定义为
\[ S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X}) \]

2. 了解 $\chi^2$ 分布、$t$ 分布、$F$ 分布的概念及性质，了解上侧 $\alpha$ 分位数的概念并会查表计算。

3. 了解正态总体的常用抽样分布。

\subsection{统计量}

随机试验的每一个可能的观察值称为个体，全部观察值称为总体，总体中包含个体的个数称为总体的容量。

$n$ 个相互独立且与总体 $X$ 同分布的随机变量 $\seq{X}{n}$ 称为来自总体 $X$ 或者来自分布函数 $F$ 的简单随机样本，其容量为 $n$。一次抽样结果的 $n$ 个具体数值称为 $\seq{x}{n}$ 的一个观测值（样本值）。

设 $\seq{x}{n}$ 为来自总体 $X$ 的一个样本，$g$ 为仅与 $x$ 有关的 $n$ 元函数，则称 $g$ 为样本的一个统计量。若 $(\seq{x}{n})$ 为样本值，则 $g(\seq{x}{n})$ 为观测值。

假设总体 $X$ 的分布函数为 $F$，则 $(\seq{x}{n})$ 的分布函数为
\[ F(\seq{x}{n}) = \prod_{i=1}^n F(x_i) \]

样本均值
\[ \overline{X} = \frac{1}{n} \sum_{i=1}^n X_i \]
样本方差
\[ S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2 \]
样本 $k$ 阶（原点）矩
\[ A_k = \frac{1}{n} \sum_{i=1}^n X_i^k \]
样本 $k$ 阶中心矩
\[ B_k = \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^k \]
将 $n$ 个观测量从小到大的顺序排列，记随机变量 $X_{(k)}$ 为第 $k$ 顺序统计量。

显然样本与总体同分布
\[ \bbE[X_i] = \bbE [X], \quad \bbD[X_i] = \bbD[X] \]
均值与方差
\[ \bbE[\overline{X}] = \bbE [X], \quad \bbD(\overline{X}) = \frac{1}{n} \bbD [X], \quad \bbE[S^2] = \bbD [X]  \]

\begin{note}
	为什么样本方差的分母是 $n-1$？因为此时 $\bbE[S^2] = \bbD [X]$，即 $S^2$ 是 $\bbD[X]$ 的无偏估计量。具体的，不难带入样本均值计算
	\[ \bbE\left[\sum_{i=1}^n \left(X_i - \frac{\sum X_i}{n}\right)^2\right]
		= \bbE\left[  \frac{n-1}{n} \left(\sum_{i=1}^n X_i^2\right) - \frac{1}{n}
			\left(\sum_{i=1}^{n} \sum_{i \neq j} X_iX_j\right) \right]
		= (n-1) \bbD[X] \]
\end{note}

\subsection{三大分布}

\paragraph{$\chi^2$ 分布}

若随机变量 $\seq{x}{n}$ 相互独立且都服从标准正态分布，则随机变量 $X = \sum X_i^2$ 服从自由度为 $n$ 的 $\chi^2$ 分布，记为 $X \sim \chi^2(n)$。其中 $\bbE[\chi^2] = n, \bbD[\chi^2] = 2n$。

对于给定的 $\alpha(0 < \alpha < 1)$，称满足
\[ \bbP\left\{\chi^2 > \chi_\alpha^2(n)\right\} = \int_{\chi_\alpha^2(n)}^n f(x) \d x = \alpha  \]
的 $\chi_\alpha^2(n)$ 为 $\chi^2(n)$ 分布的上 $\alpha$ 分位点。

\paragraph{$t$ 分布}

设随机变量 $X \sim N(0, 1), Y \sim \chi^2(n)$，$X$ 与 $Y$ 互相独立，则随机变量 $t = \frac{X}{\sqrt{Y / n}}$ 服从自由度为 $n$ 的 $t$ 分布，记为 $t \sim t(n)$.

\paragraph{$F$ 分布}

设随机变量 $X \sim \chi^2(n_1), y \sim \chi^2(n_2)$，且 $X$ 与 $Y$ 相互独立，则 $F = \frac{X / n_1}{Y / n_2}$ 服从自由度为 $(n_1, n_2)$ 的 $F$ 分布，记为 $F \sim F(n_1, n_2)$。


\section{参数估计}

\subsection{大纲要求}

1. 理解参数的点估计、估计量与估计值的概念。

2. 掌握矩估计法（一阶矩、二阶矩）和最大似然估计法。

3. 了解估计量的无偏性、有效性（最小方差性）和一致性（相合性）的概念，并会验证估计量的无偏性。

4. 理解区间估计的概念，会求单个正态总体的均值和方差的置信区间，会求两个正态总
体的均值差和方差比的置信区间。

\subsection{参数的点估计}

设总体 $X$ 的分布形式已知，但含有未知参数 $\theta$，或者总体的某数字特征存在但未知，从总体中抽取样本 $\seq{X}{n}$，相应的样本值为 $\seq{x}{n}$。借助于样本给出一个未知参数具体数值的参数估计问题就是点估计问题。

要解决点估计问题，就要构造一个适当的统计量 $\hat{\theta}(\seq{x}{n})$ 作为参数 $\theta$ 的近似值，则称 $\hat{\theta}(\seq{X}{n})$ 为 $\theta$ 的估计量，$\hat{\theta}(\seq{X}{n})$ 为 $\theta$ 为 $\theta$ 的估计值。

\subsection{矩估计法}

设总体 $X$ 分布有 $n$ 个样本，有 $k$ 个未知参数。若 $X$ 的原点矩存在，我们令样本矩等于总体矩
\[ \frac{1}{n} \sum_{i=1}^{N} X_i^l = \bbE(X^l), \quad l = 1, \cdots, k \]
这是包含 $k$ 个参数的 $k$ 个方程，由此解得矩估计量和矩估计值。

一般约定：用矩法方程求总体未知参数的估计量时，从低阶开始。

\subsection{最大似然估计法}

似然性与概率的区别：
\begin{itemize}
	\item 概率 $p(x;\theta)$ 是在已知参数 $\theta$ 的情况下，发生观测结果 $x$ 的概率；
	\item 似然性 $L(x; \theta)$ 是从观测结果 $x$ 出发，分布函数为 $\theta$ 的可能性大小。
\end{itemize}

最大似然估计值即对给定的观测值 $\seq{x}{n}$，该估计值 $\hat{\theta}$ 使似然性最大。

假设 $X$ 是离散型随机变量，其概率分布为 $\bbP\{X = x\} = p(x; \theta)$，那么求其取值概率
\[ L(\seq{x}{n};\theta) = \bbP\{X_1 = x_1, \cdots, X_n = x_n\} = \prod_{i=1}^n \bbP\{X_i=x_i\} = \prod_{i=1}^n p(x_i; \theta) \]
称为样本的似然函数。若存在 $\hat{\theta}$ 使得 $L$ 取到最大值，则称 $\hat{\theta}$ 为最大似然估计值，对应的统计量是 $\theta$ 的最大似然估计量。

同理，连续型随机变量也有
\[ L(\seq{x}{n}; \theta) = \prod_{i=1}^n f(x_i; \theta) \]

最大似然估计法步骤：
\begin{enumerate}
	\item 计算似然函数 $L(\theta)$
	\item 求关于 $\theta$ 的最大值点，一般情况下可微，求导或者对数求导
	      \[ \frac{\d L}{\d \theta} = 0, \quad \frac{\d (\ln L)}{\d \theta} = \frac{L'(\theta)}{L(\theta)} = 0 \]
	      得到极值点
	\item 用样本 $X_i$ 代替样本值 $x_i$，得到最大似然估计量 $\hat{\theta}$。
\end{enumerate}
倘若有多个参数 $\theta$，也是计算多元极值点。

\begin{note}
	注意参数 $\theta$ 的取值范围，有时候最值不是极值。
\end{note}

\subsection{估计量的评选标准}

无偏性：若估计量 $\hat{\theta}$ 的数学期望 $\bbE[\hat{\theta}]$ 存在，且 $\bbE[\hat{\theta}] = \theta$，则称 $\hat{\theta}$ 是 $\theta$ 的无偏估计量。

有效性：若 $\hat{\theta}_1$ 和 $\hat{\theta}_2$ 都是 $\theta$ 的估计量，若 $\bbD[\hat{\theta}_1] \leqslant \bbD[\hat{\theta}_2]$，则称 $\hat{\theta}_1$ 较 $\hat{\theta}_2$ 有效。
一致性（相合性）：若估计量 $\hat{\theta}$ 在 $n \to \infty$ 时 $\hat{\theta}$ 依概率收敛于 $\theta$，则称 $\hat{\theta}$ 是 $\theta$ 的一致估计量。

\section{假设检验}

\subsection{大纲要求}

1. 理解显著性检验的基本思想，掌握假设检验的基本步骤，了解假设检验可能产生的两类错误。

2. 掌握单个及两个正态总体的均值和方差的假设检验。

\subsection{假设检验的基本概念}

假设检验问题：总体函数未知，或者参数未知的情况下，提出一些关于总体分布或者参数的假设，然后抽取样本构建统计量，再根据样本所提的假设做出接受或者拒绝的决策。

检验法：借助于样本值来判断接受假设，或者拒绝假设的法则，称为检验法。

需要着重考察的假设称为原假设，常常记为 $H_0$；与原假设相对立的假设称为备选假设（或对立假设），备择假设记为 $H_1$。

检验统计量：如果基于某一个统计量的观测值来确定接受 $H_0$ 或者拒绝 $H_0$ 时，这一统计量称为检验统计量。

当统计量落在某个区域时就拒绝 $H_0$，这一区域称为拒绝域，拒绝域的边界点称为临界点。

两类错误：
\begin{itemize}
	\item 第 I 类错误：弃真，$H_0$ 实际上为真时但拒绝 $H_0$。
	\item 第 II 类错误：取伪，$H_0$ 实际上为假但接受 $H_0$。
\end{itemize}

显著性水平：在做检验时要求犯第一类错误的概率 $\leqslant \alpha$，则 $\alpha$ 称为显著性水平，通常取 $0.1, 0.05$ 等值。

显著性检验：对于给定的样本容量，只控制犯第 I 类错误的概率，而不考虑犯第 II 类错误的检验法，称为显著性检验。

假设检验的一般步骤：
\begin{itemize}
	\item 提出所要检验的原假设 $H_0$ 与备择假设 $H_1$；
	\item 选择检验的统计量，并在 $H_0$ 成立下求出它的分布；
	\item 给定显著性水平，在 $H_0$ 成立下去欸的那个临界值和否定域；
	\item 由样本值推算出统计量的值，判断该值是否落入否定域。
\end{itemize}
